# DriveGPT
Our solution leverages generative AI and large-language models (LLMs) in a novel way. We realized that multimodal LLMs can also be used for performing actions which involve spatial awareness and real-time responses through the use of image and video input, similar to how our brains work and learn to walk and drive cars through input received by our eyes and guidance of instructors. We predict a world where just like the human brain is able to learn and perform many different tasks at once, LLMs will become good enough to the point where they too will become a general-purpose AI.
In our prototype, we are using the Gemini 1.5 Flash API Multimodal LLM. For the purposes of testing, we have set up a virtual simulation of a car with realistic physics and driving dynamics, inside of an open world environment (DirectX 12). The keys of the keyboard can be used to control the car, i.e. Accelerate, brake, turn etc. Then, a python program runs in the background, captures screenshots of the simulated environment every 3 seconds. The screenshot is stored, and fed to Gemini with a complex prompt which trains the AI in detail how to drive the car. Then, Gemini returns a JSON file with the keys which need to be pressed and the duration of presses. The python script executes these actions sequentially, controlling the car. Debug output is printed to console for analytics, and errors handled by the program. Old screenshots are automatically purged.
